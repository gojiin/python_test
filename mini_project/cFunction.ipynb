{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 5, 1, 17, 15, 12, 956432)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentYear = datetime.today().year\n",
    "currentYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-05-01\n"
     ]
    }
   ],
   "source": [
    "currentMonth = datetime.today().month\n",
    "currentDate = datetime.today().day\n",
    "\n",
    "currentYear = str(currentYear)[-2:]\n",
    "\n",
    "if (currentMonth < 10):\n",
    "    currentMonth = \"0\" + str(currentMonth)\n",
    "else:\n",
    "    currentMonth = str(currentMonth)\n",
    "    \n",
    "if (currentDate < 10):\n",
    "    currentDate = \"0\" + str(currentDate)\n",
    "else:\n",
    "    currentDate = str(currentDate)\n",
    "\n",
    "\n",
    "today = currentYear + \"-\" + currentMonth + \"-\" + currentDate\n",
    "# print(currentYear + \"-\" + currentMonth + \"-\" + currentDate)\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "import os\n",
    "import requests #Used to service API connection\n",
    "from lxml import html #Used to parse XML\n",
    "from bs4 import BeautifulSoup #Used to read XML table on webpage\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import tabula \n",
    "from tabula import wrapper\n",
    "import httplib2\n",
    "import os\n",
    "import requests, bs4, pandas as pd, numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeURL(myUrl, myKey, myParameter):\n",
    "    # myUrl = \"http://192.168.1.120/index.php?\"\n",
    "    url = myUrl + myKey + \"&\" + myParameter\n",
    "\n",
    "    url = url.rstrip('&')\n",
    "    return url\n",
    "\n",
    "def xmlProcess(url):\n",
    "    response = requests.get(url)\n",
    "    # Check if page is up\n",
    "    result = []\n",
    "    if response.status_code == 200:\n",
    "        # Convert webpage to %Data\n",
    "        Data = BeautifulSoup(response.text, 'lxml-xml')\n",
    "        rows = 0\n",
    "        columnName = []\n",
    "        # search Item all item tag\n",
    "        iterData = Data.find_all('item')\n",
    "        for item in iterData:\n",
    "            item_list = []\n",
    "            # Fill the value in one row\n",
    "            for tag in item.find_all():\n",
    "                try:\n",
    "                    tagname = tag.name\n",
    "                    if rows == 0:\n",
    "                        columnName.append(tagname)\n",
    "                    item_list.append(item.find(tagname).text)\n",
    "                except Exception as e:\n",
    "                    print(\"This row will be ignored. \", item_list)\n",
    "            rows = rows + 1\n",
    "            result.append(item_list)\n",
    "    finalResult = pd.DataFrame(result)\n",
    "    finalResult.columns = columnName\n",
    "    print(finalResult)\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "def jsonProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    # JSON 데이터 획득\n",
    "    json = response.json()\n",
    "    # PandasDataframe변환\n",
    "    df = json_normalize(json)\n",
    "    return df\n",
    "\n",
    "def csvProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_csv(url, encoding=\"ms949\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdfProcess(inputFolder, url):\n",
    "    #다른 사이트에서 pdf 데이터를 받을 때는 함수를 다시 만들어야 해요\n",
    "    #response = requests.get(\"http://fsc.go.kr/info/trd_list.jsp?menu=7230000&bbsid=BBS0069\")\n",
    "    resp = requests.get(\"http://fsc.go.kr/info/trd_list.jsp?menu=7230000&bbsid=BBS0069\")\n",
    "    #response = requests.get(\"http://fsc.go.kr/info/trd_list.jsp?menu=7230000&bbsid=BBS0069\")\n",
    "    resp.encoding='utf-8'\n",
    "    html = resp.text\n",
    "    bs = bs4.BeautifulSoup(html, 'html.parser')\n",
    "   \n",
    "    #데이터 추출하기\n",
    "    originalData  = bs.select(\"#contents > div.board > table > tbody > tr > td > a\")\n",
    "    convertedData = str(originalData[2])\n",
    "    dailyUpdated = convertedData.split('\"')[1].split(\"amp;\")[1]\n",
    "\n",
    "    url = url + \"&\" + dailyUpdated # & 를 안 써줘서 직업이 안됐었어....\n",
    "    df2 = wrapper.read_pdf(url,\n",
    "              multiple_tables=True,\n",
    "              pages=\"all\",\n",
    "              pandas_options={\"header\":0})\n",
    "    #파일이름에 반영할 현재 날짜 구하기 \n",
    "    currentYear = datetime.today().year\n",
    "    currentMonth = datetime.today().month\n",
    "    currentDate = datetime.today().day\n",
    "\n",
    "    currentYear = str(currentYear)[-2:]\n",
    "    if (currentMonth < 10):\n",
    "        currentMonth = \"0\" + str(currentMonth)\n",
    "    else:\n",
    "        currentMonth = str(currentMonth)\n",
    "\n",
    "    if (currentDate < 10):\n",
    "        currentDate = \"0\" + str(currentDate)\n",
    "    else:\n",
    "        currentDate = str(currentDate)\n",
    "\n",
    "\n",
    "    today = currentYear + currentMonth + currentDate\n",
    "    \n",
    "    #각각의 표를 파일로 저장\n",
    "    for i in range(0, len(df2)):\n",
    "        fileName = inputFolder + \"_\" + today + \"_\" + str(i) + '.csv'\n",
    "        df2[i].to_csv(\"../../data/outbound/\" + inputFolder + \"/\" + fileName, index=False, encoding = \"ms949\")\n",
    "        \n",
    "    #continue\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "import os\n",
    "import requests #Used to service API connection\n",
    "from lxml import html #Used to parse XML\n",
    "from bs4 import BeautifulSoup #Used to read XML table on webpage\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import tabula \n",
    "from tabula import wrapper\n",
    "import httplib2\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def makeURL(myUrl, myKey, myParameter):\n",
    "    # myUrl = \"http://192.168.1.120/index.php?\"\n",
    "    url = myUrl + myKey + \"&\" + myParameter\n",
    "\n",
    "    url = url.rstrip('&')\n",
    "    return url\n",
    "\n",
    "def xmlProcess(url):\n",
    "    response = requests.get(url)\n",
    "    # Check if page is up\n",
    "    result = []\n",
    "    if response.status_code == 200:\n",
    "        # Convert webpage to %Data\n",
    "        Data = BeautifulSoup(response.text, 'lxml-xml')\n",
    "        rows = 0\n",
    "        columnName = []\n",
    "        # search Item all item tag\n",
    "        iterData = Data.find_all('item')\n",
    "        for item in iterData:\n",
    "            item_list = []\n",
    "            # Fill the value in one row\n",
    "            for tag in item.find_all():\n",
    "                try:\n",
    "                    tagname = tag.name\n",
    "                    if rows == 0:\n",
    "                        columnName.append(tagname)\n",
    "                    item_list.append(item.find(tagname).text)\n",
    "                except Exception as e:\n",
    "                    print(\"This row will be ignored. \", item_list)\n",
    "            rows = rows + 1\n",
    "            result.append(item_list)\n",
    "    finalResult = pd.DataFrame(result)\n",
    "    finalResult.columns = columnName\n",
    "    print(finalResult)\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "def jsonProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    # JSON 데이터 획득\n",
    "    json = response.json()\n",
    "    # PandasDataframe변환\n",
    "    df = json_normalize(json)\n",
    "    return df\n",
    "\n",
    "def csvProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    df = pd.read_csv(url, encoding=\"ms949\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdfProcess(inputFolder, url):\n",
    "    #다른 사이트에서 pdf 데이터를 받을 때는 함수를 다시 만들어야 해요\n",
    "    #response = requests.get(\"http://fsc.go.kr/info/trd_list.jsp?menu=7230000&bbsid=BBS0069\")\n",
    "    response = requests.get(url)\n",
    "    response.encoding='utf-8'\n",
    "    html = response.text\n",
    "    originalData  = bs.select(\"#contents > div.board > table > tbody > tr > td > a\")\n",
    "    convertedData = str(originalData[2])\n",
    "    dailyUpdated = convertedData.split('\"')[1].split(\"amp;\")[1]\n",
    "\n",
    "    url = \"http://fsc.go.kr\" + dailyUpdated\n",
    "    bs = bs4.BeautifulSoup(html, 'html.parser')\n",
    "    df2 = wrapper.read_pdf(url,\n",
    "              multiple_tables=True,\n",
    "              pages=\"all\",\n",
    "              pandas_options={\"header\":0})\n",
    "    #각각의 표를 파일로 저장\n",
    "    for i in range(0, len(df2)):\n",
    "        fileName = inputFolder + str(i) + '.csv'\n",
    "        df2[i].to_csv(\"../../data/outbound/\" + inputFolder + \"/\" + fileName, index=False, encoding = \"ms949\")\n",
    "        \n",
    "    #continue\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests #Used to service API connection\n",
    "from lxml import html #Used to parse XML\n",
    "from bs4 import BeautifulSoup #Used to read XML table on webpage\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import tabula\n",
    "from tabula import wrapper\n",
    "\n",
    "\n",
    "def makeURL(myUrl, myKey, myParameter):\n",
    "    # myUrl = \"http://192.168.1.120/index.php?\"\n",
    "    url = myUrl + myKey + \"&\" + myParameter\n",
    "\n",
    "    url = url.rstrip('&')\n",
    "    return url\n",
    "\n",
    "def xmlProcess(url):\n",
    "    response = requests.get(url)\n",
    "    # Check if page is up\n",
    "    result = []\n",
    "    if response.status_code == 200:\n",
    "        # Convert webpage to %Data\n",
    "        Data = BeautifulSoup(response.text, 'lxml-xml')\n",
    "        rows = 0\n",
    "        columnName = []\n",
    "        # search Item all item tag\n",
    "        iterData = Data.find_all('item')\n",
    "        for item in iterData:\n",
    "            item_list = []\n",
    "            # Fill the value in one row\n",
    "            for tag in item.find_all():\n",
    "                try:\n",
    "                    tagname = tag.name\n",
    "                    if rows == 0:\n",
    "                        columnName.append(tagname)\n",
    "                    item_list.append(item.find(tagname).text)\n",
    "                except Exception as e:\n",
    "                    print(\"This row will be ignored. \", item_list)\n",
    "            rows = rows + 1\n",
    "            result.append(item_list)\n",
    "    finalResult = pd.DataFrame(result)\n",
    "    finalResult.columns = columnName\n",
    "    print(finalResult)\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "def jsonProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    # JSON 데이터 획득\n",
    "    json = response.json()\n",
    "    # PandasDataframe변환\n",
    "    df = json_normalize(json)\n",
    "    return df\n",
    "\n",
    "def csvProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    df = pd.read_csv(url, encoding=\"ms949\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdfProcess(url):\n",
    "    response = requests.get(url)\n",
    "    df2 = wrapper.read_pdf(url,\n",
    "              multiple_tables=True,\n",
    "              pages=\"all\",\n",
    "              pandas_options={\"header\":0})\n",
    "    #각각의 표를 파일로 저장\n",
    "    for i in range(0, len(df2)):\n",
    "        fileName = '금융시장동향' + str(i) + '.csv'\n",
    "        df = df2[i].to_csv(\"../../data/outbound/\" + fileName, index=False, encoding = \"ms949\")\n",
    "        \n",
    "        return df \n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "import os\n",
    "import requests #Used to service API connection\n",
    "from lxml import html #Used to parse XML\n",
    "from bs4 import BeautifulSoup #Used to read XML table on webpage\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import tabula \n",
    "from tabula import wrapper\n",
    "\n",
    "def makeURL(myUrl, myKey, myParameter):\n",
    "    # myUrl = \"http://192.168.1.120/index.php?\"\n",
    "    url = myUrl + myKey + \"&\" + myParameter\n",
    "\n",
    "    url = url.rstrip('&')\n",
    "    return url\n",
    "\n",
    "def xmlProcess(url):\n",
    "    response = requests.get(url)\n",
    "    # Check if page is up\n",
    "    result = []\n",
    "    if response.status_code == 200:\n",
    "        # Convert webpage to %Data\n",
    "        Data = BeautifulSoup(response.text, 'lxml-xml')\n",
    "        rows = 0\n",
    "        columnName = []\n",
    "        # search Item all item tag\n",
    "        iterData = Data.find_all('item')\n",
    "        for item in iterData:\n",
    "            item_list = []\n",
    "            # Fill the value in one row\n",
    "            for tag in item.find_all():\n",
    "                try:\n",
    "                    tagname = tag.name\n",
    "                    if rows == 0:\n",
    "                        columnName.append(tagname)\n",
    "                    item_list.append(item.find(tagname).text)\n",
    "                except Exception as e:\n",
    "                    print(\"This row will be ignored. \", item_list)\n",
    "            rows = rows + 1\n",
    "            result.append(item_list)\n",
    "    finalResult = pd.DataFrame(result)\n",
    "    finalResult.columns = columnName\n",
    "    print(finalResult)\n",
    "    return finalResult\n",
    "\n",
    "\n",
    "def jsonProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "    # JSON 데이터 획득\n",
    "    json = response.json()\n",
    "    # PandasDataframe변환\n",
    "    df = json_normalize(json)\n",
    "    return df\n",
    "\n",
    "def csvProcess(url):\n",
    "\n",
    "    # 정상 여부 확인 (200 정상)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    df = pd.read_csv(url, encoding=\"ms949\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def pdfProcess(url):\n",
    "    response = requests.get(url)\n",
    "    df2 = wrapper.read_pdf(url,\n",
    "              multiple_tables=True,\n",
    "              pages=\"all\",\n",
    "              pandas_options={\"header\":0})\n",
    "    #각각의 표를 파일로 저장\n",
    "    for i in range(0, len(df2)):\n",
    "        fileName = '금융시장동향' + str(i) + '.csv'\n",
    "        df2[i].to_csv(\"../../data/outbound/\" +  inputFolder + \"/\" + fileName, index=False, encoding = \"ms949\")\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
